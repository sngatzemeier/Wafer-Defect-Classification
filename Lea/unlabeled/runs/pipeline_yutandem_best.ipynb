{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "**Data preprocessing**: Resize to 224x224, binarize, and apply n=2 morphological thinning (detect model); Resized to 224x224 with no filters (classify model)\n",
    "\n",
    "**Model**: Tandem inference implementation of the detection and classification model from the Yu, et al [paper](https://drive.google.com/file/d/1nYl4w41CAcj8XwTEdVwcD5lVheUFIHVy/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pickle5 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (0.0.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17003,
     "status": "ok",
     "timestamp": 1645130034371,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgzyTYobr3WBfcW_CbeVl1vthXNBKocBj7_WOpm=s64",
      "userId": "08012892845319420981"
     },
     "user_tz": 300
    },
    "id": "MMcWUHT5-eVD",
    "outputId": "00111a49-0a3c-4392-f905-805f1030b923"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models, losses, optimizers, regularizers, callbacks\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import cv2\n",
    "from scipy.ndimage import median_filter\n",
    "from skimage.transform import resize as sk_resize\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.morphology import skeletonize, thin\n",
    "\n",
    "import pipeline_helpers as helper\n",
    "\n",
    "import boto3\n",
    "import pickle5 as pickle\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'wafer-capstone'\n",
    "my_bucket = s3.Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices(device_type=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "Dataset must have the following columns: \n",
    "- **waferMap**: defect data of wafer map where 0 = blank spot, 1 = normal die (passed the electrical test), and 2 = broken die (failed electrical test)\n",
    "- **ID**: unique identification for each waferMap, separate from dataframe index\n",
    "\n",
    "If labeled, dataset must have the following columns:\n",
    "- **detectLabels**: for evaluating the detect model, where 0 = no defect, 1 = defect\n",
    "- **classifyLabels**: for evaluating the classify model, where 0 = Loc, 1 = Edge-Loc, 2 = Center, 3 = Edge-Ring, 4 = Scratch, 5 = Random, 6 = Near-full, 7 = Donut, 8 = none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify variables\n",
    "\n",
    "# specify data to load\n",
    "path = 'processed_data/customer'\n",
    "filename = 'WM-unlabeled-random50-424'\n",
    "labeled = False\n",
    "\n",
    "# where to save results\n",
    "result_path = 'results'\n",
    "result_filename = 'yutandem-best-unlabeled-random424'\n",
    "\n",
    "# which models to run\n",
    "detect_model = 'yudetect-224-thin2'\n",
    "classify_model = 'yuclassify-224'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0.23 seconds\n",
      "Dataset length: 1250\n"
     ]
    }
   ],
   "source": [
    "# load data directly from S3 (using boto3 resource)\n",
    "start = time.time()\n",
    "\n",
    "data_key = f'{path}/{filename}.pkl'\n",
    "data = pickle.loads(my_bucket.Object(data_key).get()['Body'].read())\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))\n",
    "print(f\"Dataset length: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF LABELED\n",
    "# show failure type distribution\n",
    "if labeled:\n",
    "    data_defects = data[data.detectLabels == 1]\n",
    "    helper.defect_distribution(data_defects, note=f'({filename})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.74 seconds\n",
      "Sanity checks:\n",
      "Map shape: (224, 224)\n"
     ]
    }
   ],
   "source": [
    "# resize to 224x224\n",
    "start = time.time()\n",
    "\n",
    "def resize(x):\n",
    "    y = sk_resize(x, [224,224])\n",
    "    new_y = img_as_ubyte(y)\n",
    "    return new_y\n",
    "    \n",
    "data['waferMap224'] = data.waferMap.apply(lambda x: resize(x))\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))\n",
    "print(\"Sanity checks:\")\n",
    "print(f'Map shape: {data.waferMap224[0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.80 seconds\n",
      "Sanity checks:\n",
      "Map shape: (224, 224)\n"
     ]
    }
   ],
   "source": [
    "# resize to 224x224 --> binarize --> apply n=2 morphological thinning\n",
    "start = time.time()\n",
    "\n",
    "def preprocess(x):\n",
    "    ret, thresh_img = cv2.threshold(x, 1, 1, cv2.THRESH_BINARY)\n",
    "    thin_img = thin(thresh_img, 2)\n",
    "    return thin_img\n",
    "    \n",
    "data['thinMap2'] = data.waferMap224.apply(lambda x: preprocess(x))\n",
    "data['thinMap2'] = data.thinMap2.apply(lambda x: x.astype(np.uint8))\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))\n",
    "print(\"Sanity checks:\")\n",
    "print(f'Map shape: {data.thinMap2[0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detect data set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 842,
     "status": "ok",
     "timestamp": 1645130310224,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgzyTYobr3WBfcW_CbeVl1vthXNBKocBj7_WOpm=s64",
      "userId": "08012892845319420981"
     },
     "user_tz": 300
    },
    "id": "poThmiyAoHSi",
    "outputId": "b93c01b0-692b-4344-ce11-61da3613f166"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.00 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1250, 224, 224, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare inputs\n",
    "start = time.time()\n",
    "\n",
    "x_det = np.stack(data['thinMap2'])\n",
    "x_det = tf.expand_dims(x_det, axis=3, name=None)\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))\n",
    "# sanity check\n",
    "# expected: TensorShape([#rows, xdim, ydim, 1])\n",
    "x_det.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rIS8zJfn_aDO"
   },
   "outputs": [],
   "source": [
    "# IF LABELED\n",
    "# prepare labels for evaluating results\n",
    "if labeled:\n",
    "    y_det = np.asarray(data['detectLabels']).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and run detect model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 224, 224, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 74, 74, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               1048832   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 1,105,090\n",
      "Trainable params: 1,105,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Wall time: 0.78 seconds\n"
     ]
    }
   ],
   "source": [
    "# load saved detect model from S3\n",
    "start = time.time()\n",
    "\n",
    "detect = helper.s3_get_keras_model(detect_model)\n",
    "detect.summary()\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.25 seconds\n"
     ]
    }
   ],
   "source": [
    "# generate predictions\n",
    "start = time.time()\n",
    "\n",
    "detect_pred = detect.predict(x_det)\n",
    "det_labels = np.argmax(detect_pred, axis=1).astype(np.uint8)\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF LABELED\n",
    "# evaluate detect model performance\n",
    "if labeled:\n",
    "    \n",
    "    # calculate baseline accuracy\n",
    "    nones = len(data[data.detectLabels == 0])\n",
    "    total = len(data)\n",
    "    print(f\"Baseline accuracy: {nones/total*100:.2f}%\")\n",
    "    \n",
    "    # manually compute detect model accuracy\n",
    "    det_cm = confusion_matrix(y_det, det_labels)\n",
    "    det_accuracy = (det_cm[0][0] + det_cm[1][1]) / len(y_det) * 100\n",
    "    print(f'Detection Model Accuracy: {det_accuracy:.2f}%')\n",
    "    \n",
    "    # plot confusion matrix\n",
    "    helper.plot_confusion_matrix(y_det, det_labels, mode='detect', normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classify data set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity checks:\n",
      "74, 74, (74, 7)\n"
     ]
    }
   ],
   "source": [
    "# keep only subset of test data\n",
    "# predicted by detect model as having defects\n",
    "defect_indices = [i for i in range(len(det_labels)) if det_labels[i] == 1]\n",
    "defect_ids = [data.ID[i] for i in defect_indices]\n",
    "defect_df = data.loc[defect_indices].reset_index(drop=True)\n",
    "\n",
    "# sanity check:\n",
    "print(\"Sanity checks:\")\n",
    "print(f'{len(defect_indices)}, {len(defect_ids)}, {defect_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0.01 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([74, 224, 224, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare inputs\n",
    "start = time.time()\n",
    "\n",
    "x_cls = np.stack(defect_df['waferMap224'])\n",
    "x_cls = tf.expand_dims(x_cls, axis=3, name=None)\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))\n",
    "# sanity check\n",
    "# expected: TensorShape([#rows, xdim, ydim, 1])\n",
    "x_cls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF LABELED\n",
    "# prepare labels for evaluating results\n",
    "if labeled:\n",
    "    y_cls = np.asarray(defect_df['classifyLabels']).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and run classify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rescaling (Rescaling)        (None, 224, 224, 1)       0         \n",
      "_________________________________________________________________\n",
      "random_flip (RandomFlip)     (None, 224, 224, 1)       0         \n",
      "_________________________________________________________________\n",
      "random_rotation (RandomRotat (None, 224, 224, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 224, 224, 32)      320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 74, 74, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 74, 74, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 24, 24, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 24, 24, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              33558528  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 8200      \n",
      "=================================================================\n",
      "Total params: 38,196,072\n",
      "Trainable params: 38,196,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Wall time: 10.79 seconds\n"
     ]
    }
   ],
   "source": [
    "# load saved classify model from S3\n",
    "start = time.time()\n",
    "\n",
    "classify = helper.s3_get_keras_model(classify_model)\n",
    "classify.summary()\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0.49 seconds\n"
     ]
    }
   ],
   "source": [
    "# generate predictions\n",
    "start = time.time()\n",
    "\n",
    "classify_pred = classify.predict(x_cls)\n",
    "cls_labels = np.argmax(classify_pred, axis=1).astype(np.uint8)\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect tandem model results\n",
    "Saved predictions include 4 lists:\n",
    "- IDs of defective wafers identified by detect model\n",
    "- Output of detect model (softmax probabilities)\n",
    "- Output of classify model (softmax probabilities)\n",
    "- Labels predicted by tandem model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250\n"
     ]
    }
   ],
   "source": [
    "# generate full prediction\n",
    "def tandem_prediction(x):\n",
    "    if x in set(defect_ids):\n",
    "        i = defect_ids.index(x)\n",
    "        return cls_labels[i]\n",
    "    else:\n",
    "        return 8\n",
    "\n",
    "data['tandemLabels'] = data.ID.apply(lambda x: tandem_prediction(x))\n",
    "tandem_pred = data['tandemLabels'].tolist()\n",
    "print(len(tandem_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions to local instance\n",
    "predictions = [defect_ids, detect_pred, classify_pred, tandem_pred]\n",
    "with open(f'{result_path}/{result_filename}.pkl', \"wb\") as f:\n",
    "    pickle.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF LABELED\n",
    "if labeled:\n",
    "    y_test = data['classifyLabels'].tolist()\n",
    "    \n",
    "    # manually compute overall accuracy\n",
    "    tandem_cm = confusion_matrix(y_test, tandem_pred)\n",
    "\n",
    "    tandem_num = 0\n",
    "    for i in range(9):\n",
    "        tandem_num += tandem_cm[i][i]\n",
    "\n",
    "    overall_accuracy = tandem_num / len(y_test) * 100\n",
    "    print(f'Overall Model Accuracy: {overall_accuracy:.2f}%') \n",
    "\n",
    "    # plot confusion matrix\n",
    "    helper.plot_confusion_matrix(y_test, tandem_pred, mode='all', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF LABELED\n",
    "# plot confusion matrix counts\n",
    "if labeled:\n",
    "    helper.plot_confusion_matrix(y_test, tandem_pred, mode='all', normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "colab_lenet_classify_all2.ipynb",
   "provenance": [
    {
     "file_id": "https://gist.github.com/mrgrhn/c6d2a157ebfc883e462f2d6e2ce2e3ce#file-lenet_tensorflow-ipynb",
     "timestamp": 1645028600895
    }
   ]
  },
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
