{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "**Data preprocessing**: Resize to 224x224, binarize, and apply n=2 morphological thinning (detect model); Resized to 224x224 with no filters (classify model)\n",
    "\n",
    "**Model**: Tandem inference implementation of the detection and classification model from the Yu, et al [paper](https://drive.google.com/file/d/1nYl4w41CAcj8XwTEdVwcD5lVheUFIHVy/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17003,
     "status": "ok",
     "timestamp": 1645130034371,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgzyTYobr3WBfcW_CbeVl1vthXNBKocBj7_WOpm=s64",
      "userId": "08012892845319420981"
     },
     "user_tz": 300
    },
    "id": "MMcWUHT5-eVD",
    "outputId": "00111a49-0a3c-4392-f905-805f1030b923"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models, losses, optimizers, regularizers, callbacks\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import cv2\n",
    "from scipy.ndimage import median_filter\n",
    "from skimage.transform import resize as sk_resize\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.morphology import skeletonize, thin\n",
    "\n",
    "import helpers as helper\n",
    "from keras_model_s3_wrapper import *\n",
    "\n",
    "import boto3\n",
    "import pickle5 as pickle\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'wafer-capstone'\n",
    "my_bucket = s3.Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices(device_type=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "Dataset must have the following columns: \n",
    "- **waferMap**: defect data of wafer map where 0 = blank spot, 1 = normal die (passed the electrical test), and 2 = broken die (failed electrical test)\n",
    "- **ID**: unique identification for each waferMap, separate from dataframe index\n",
    "\n",
    "If labeled, dataset must have the following columns:\n",
    "- **detectLabels**: for evaluating the detect model, where 0 = no defect, 1 = defect\n",
    "- **classifyLabels**: for evaluating the classify model, where 0 = Loc, 1 = Edge-Loc, 2 = Center, 3 = Edge-Ring, 4 = Scratch, 5 = Random, 6 = Near-full, 7 = Donut, 8 = none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify variables\n",
    "\n",
    "# specify data to load\n",
    "path = '' # S3 folder where data lives\n",
    "filename = '' # data filename in S3\n",
    "labeled = True\n",
    "\n",
    "# where to save results\n",
    "result_path = '' # folder in local instance to save results\n",
    "result_filename = '' # filename to save the results as\n",
    "\n",
    "# which models to run\n",
    "detect_model = 'yudetect-224-thin2'\n",
    "classify_model = 'yuclassify-224'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data directly from S3 (using boto3 resource)\n",
    "start = time.time()\n",
    "\n",
    "data_key = f'{path}/{filename}.pkl'\n",
    "data = pickle.loads(my_bucket.Object(data_key).get()['Body'].read())\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))\n",
    "print(f\"Dataset length: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF LABELED\n",
    "# show failure type distribution\n",
    "if labeled:\n",
    "    data_defects = data[data.detectLabels == 1]\n",
    "    helper.defect_distribution(data_defects, note=f'({filename})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize to 224x224\n",
    "start = time.time()\n",
    "\n",
    "def resize(x):\n",
    "    y = sk_resize(x, [224,224])\n",
    "    new_y = img_as_ubyte(y)\n",
    "    return new_y\n",
    "    \n",
    "data['waferMap224'] = data.waferMap.apply(lambda x: resize(x))\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))\n",
    "print(\"Sanity checks:\")\n",
    "print(f'Map shape: {data.waferMap224[0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize to 224x224 --> binarize --> apply n=2 morphological thinning\n",
    "start = time.time()\n",
    "\n",
    "def preprocess(x):\n",
    "    ret, thresh_img = cv2.threshold(x, 1, 1, cv2.THRESH_BINARY)\n",
    "    thin_img = thin(thresh_img, 2)\n",
    "    return thin_img\n",
    "    \n",
    "data['thinMap2'] = data.waferMap224.apply(lambda x: preprocess(x))\n",
    "data['thinMap2'] = data.thinMap2.apply(lambda x: x.astype(np.uint8))\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))\n",
    "print(\"Sanity checks:\")\n",
    "print(f'Map shape: {data.thinMap2[0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detect data set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 842,
     "status": "ok",
     "timestamp": 1645130310224,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgzyTYobr3WBfcW_CbeVl1vthXNBKocBj7_WOpm=s64",
      "userId": "08012892845319420981"
     },
     "user_tz": 300
    },
    "id": "poThmiyAoHSi",
    "outputId": "b93c01b0-692b-4344-ce11-61da3613f166"
   },
   "outputs": [],
   "source": [
    "# prepare inputs\n",
    "start = time.time()\n",
    "\n",
    "x_det = np.stack(data['thinMap2'])\n",
    "x_det = tf.expand_dims(x_det, axis=3, name=None)\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))\n",
    "# sanity check\n",
    "# expected: TensorShape([#rows, xdim, ydim, 1])\n",
    "x_det.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIS8zJfn_aDO"
   },
   "outputs": [],
   "source": [
    "# IF LABELED\n",
    "# prepare labels for evaluating results\n",
    "if labeled:\n",
    "    y_det = np.asarray(data['detectLabels']).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and run detect model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved detect model from S3\n",
    "start = time.time()\n",
    "\n",
    "detect = s3_get_keras_model(detect_model)\n",
    "detect.summary()\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions\n",
    "start = time.time()\n",
    "\n",
    "detect_pred = detect.predict(x_det)\n",
    "det_labels = np.argmax(detect_pred, axis=1).astype(np.uint8)\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF LABELED\n",
    "# evaluate detect model performance\n",
    "if labeled:\n",
    "    \n",
    "    # calculate baseline accuracy\n",
    "    nones = len(data[data.detectLabels == 0])\n",
    "    total = len(data)\n",
    "    print(f\"Baseline accuracy: {nones/total*100:.2f}%\")\n",
    "    \n",
    "    # manually compute detect model accuracy\n",
    "    det_cm = confusion_matrix(y_det, det_labels)\n",
    "    det_accuracy = (det_cm[0][0] + det_cm[1][1]) / len(y_det) * 100\n",
    "    print(f'Detection Model Accuracy: {det_accuracy:.2f}%')\n",
    "    \n",
    "    # plot confusion matrix\n",
    "    helper.plot_confusion_matrix(y_det, det_labels, mode='detect', normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classify data set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only subset of test data\n",
    "# predicted by detect model as having defects\n",
    "defect_indices = [i for i in range(len(det_labels)) if det_labels[i] == 1]\n",
    "defect_ids = [data.ID[i] for i in defect_indices]\n",
    "defect_df = data.loc[defect_indices].reset_index(drop=True)\n",
    "\n",
    "# sanity check:\n",
    "print(\"Sanity checks:\")\n",
    "print(f'{len(defect_indices)}, {len(defect_ids)}, {defect_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare inputs\n",
    "start = time.time()\n",
    "\n",
    "x_cls = np.stack(defect_df['waferMap224'])\n",
    "x_cls = tf.expand_dims(x_cls, axis=3, name=None)\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))\n",
    "# sanity check\n",
    "# expected: TensorShape([#rows, xdim, ydim, 1])\n",
    "x_cls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF LABELED\n",
    "# prepare labels for evaluating results\n",
    "if labeled:\n",
    "    y_cls = np.asarray(defect_df['classifyLabels']).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and run classify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved classify model from S3\n",
    "start = time.time()\n",
    "\n",
    "classify = s3_get_keras_model(classify_model)\n",
    "classify.summary()\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions\n",
    "start = time.time()\n",
    "\n",
    "classify_pred = classify.predict(x_cls)\n",
    "cls_labels = np.argmax(classify_pred, axis=1).astype(np.uint8)\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect tandem model results\n",
    "Saved predictions include 4 lists:\n",
    "- IDs of defective wafers identified by detect model\n",
    "- Output of detect model (softmax probabilities)\n",
    "- Output of classify model (softmax probabilities)\n",
    "- Labels predicted by tandem model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate full prediction\n",
    "def tandem_prediction(x):\n",
    "    if x in set(defect_ids):\n",
    "        i = defect_ids.index(x)\n",
    "        return cls_labels[i]\n",
    "    else:\n",
    "        return 8\n",
    "\n",
    "data['tandemLabels'] = data.ID.apply(lambda x: tandem_prediction(x))\n",
    "tandem_pred = data['tandemLabels'].tolist()\n",
    "print(len(tandem_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions to local instance\n",
    "predictions = [defect_ids, detect_pred, classify_pred, tandem_pred]\n",
    "with open(f'{result_path}/{result_filename}.pkl', \"wb\") as f:\n",
    "    pickle.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF LABELED\n",
    "if labeled:\n",
    "    y_test = data['classifyLabels'].tolist()\n",
    "    \n",
    "    # manually compute overall accuracy\n",
    "    tandem_cm = confusion_matrix(y_test, tandem_pred)\n",
    "\n",
    "    tandem_num = 0\n",
    "    for i in range(9):\n",
    "        tandem_num += tandem_cm[i][i]\n",
    "\n",
    "    overall_accuracy = tandem_num / len(y_test) * 100\n",
    "    print(f'Overall Model Accuracy: {overall_accuracy:.2f}%') \n",
    "\n",
    "    # plot confusion matrix\n",
    "    helper.plot_confusion_matrix(y_test, tandem_pred, mode='all', normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF LABELED\n",
    "# plot confusion matrix counts\n",
    "if labeled:\n",
    "    helper.plot_confusion_matrix(y_test, tandem_pred, mode='all', normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optional visualization of misclassified wafers\n",
    "Parameters:\n",
    "- **true_label**: true label of the wafer\n",
    "- **pred_label**: label predicted by the model\n",
    "- **n**: number of samples to visualize (note: must be less than or equal to the total number in confusion matrix)\n",
    "\n",
    "0 = Loc, 1 = Edge-Loc, 2 = Center, 3 = Edge-Ring, 4 = Scratch, 5 = Random, 6 = Near-full, 7 = Donut, 8 = none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot mislabeled wafers\n",
    "# print('Scratch mislabled as None')\n",
    "# helper.visualize_misclassified(data, y_test, tandem_pred, true_label=4, pred_label=8, n=9, \n",
    "#                         figsize=(5,5), col='waferMap', cmap='gray_r')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "colab_lenet_classify_all2.ipynb",
   "provenance": [
    {
     "file_id": "https://gist.github.com/mrgrhn/c6d2a157ebfc883e462f2d6e2ce2e3ce#file-lenet_tensorflow-ipynb",
     "timestamp": 1645028600895
    }
   ]
  },
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
