{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "**Data preprocessing**: Resize to 52x52, no filter\n",
    "\n",
    "**Model**: Implementation of the classification model from Junliangwangdhu [GitHub](https://github.com/Junliangwangdhu/WaferMap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas==1.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17003,
     "status": "ok",
     "timestamp": 1645130034371,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgzyTYobr3WBfcW_CbeVl1vthXNBKocBj7_WOpm=s64",
      "userId": "08012892845319420981"
     },
     "user_tz": 300
    },
    "id": "MMcWUHT5-eVD",
    "outputId": "00111a49-0a3c-4392-f905-805f1030b923"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras import datasets, layers, models, losses, optimizers, regularizers, callbacks, Input, Model, Sequential\n",
    "from keras import datasets, layers, models, losses, optimizers, regularizers, callbacks, Input, Model, Sequential\n",
    "\n",
    "from layers_train import ConvOffset2D_train\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import cv2\n",
    "from scipy.ndimage import median_filter\n",
    "from skimage.transform import resize as sk_resize\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.morphology import skeletonize, thin\n",
    "\n",
    "import helpers as helper\n",
    "from keras_model_s3_wrapper import *\n",
    "\n",
    "import boto3\n",
    "import pickle5 as pickle\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'wafer-capstone'\n",
    "my_bucket = s3.Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "Dataset must have the following columns: \n",
    "- **waferMap**: defect data of wafer map where 0 = blank spot, 1 = normal die (passed the electrical test), and 2 = broken die (failed electrical test)\n",
    "- **ID**: unique identification for each waferMap, separate from dataframe index\n",
    "\n",
    "If labeled, dataset must have the following columns:\n",
    "- **detectLabels**: for evaluating the detect model, where 0 = no defect, 1 = defect\n",
    "- **classifyLabels**: for evaluating the classify model, where 0 = Loc, 1 = Edge-Loc, 2 = Center, 3 = Edge-Ring, 4 = Scratch, 5 = Random, 6 = Near-full, 7 = Donut, 8 = none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify variables\n",
    "\n",
    "# specify data to load\n",
    "path = 'processed_data/customer'\n",
    "filename = 'MixedWM38-single'\n",
    "labeled = True\n",
    "\n",
    "# # where to save results\n",
    "# result_path = ''\n",
    "# result_filename = ''\n",
    "\n",
    "# which models to run\n",
    "classify_model = 'wmdcn-52thin-undersampled.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data directly from S3 (using boto3 resource)\n",
    "start = time.time()\n",
    "\n",
    "data_key = f'{path}/{filename}.pkl'\n",
    "data = pickle.loads(my_bucket.Object(data_key).get()['Body'].read())\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))\n",
    "print(f\"Dataset length: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF LABELED\n",
    "# show failure type distribution\n",
    "if labeled:\n",
    "    data_defects = data[data.detectLabels == 1]\n",
    "    helper.defect_distribution(data_defects, note=f'({filename})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize to 52x52\n",
    "start = time.time()\n",
    "\n",
    "def preprocess(x):\n",
    "    y = sk_resize(x, [52,52])\n",
    "    new_y = img_as_ubyte(y)\n",
    "    ret, thresh_img = cv2.threshold(new_y, 1, 1, cv2.THRESH_BINARY)\n",
    "    z = thin(thresh_img, 2).astype(np.uint8)\n",
    "    return z\n",
    "    \n",
    "data['waferMap224'] = data.waferMap.apply(lambda x: preprocess(x))\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))\n",
    "print(\"Sanity checks:\")\n",
    "print(f'Map shape: {data.waferMap224[0].shape}')\n",
    "print(f'Map unique values:{np.unique(data.waferMap224[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data.waferMap224[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 842,
     "status": "ok",
     "timestamp": 1645130310224,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgzyTYobr3WBfcW_CbeVl1vthXNBKocBj7_WOpm=s64",
      "userId": "08012892845319420981"
     },
     "user_tz": 300
    },
    "id": "poThmiyAoHSi",
    "outputId": "b93c01b0-692b-4344-ce11-61da3613f166"
   },
   "outputs": [],
   "source": [
    "# prepare inputs\n",
    "start = time.time()\n",
    "\n",
    "x_cls = np.stack(data['waferMap224'])\n",
    "x_cls = np.expand_dims(x_cls, axis=-1)\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))\n",
    "# sanity check\n",
    "# expected: TensorShape([#rows, xdim, ydim, 1])\n",
    "x_cls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIS8zJfn_aDO"
   },
   "outputs": [],
   "source": [
    "# IF LABELED\n",
    "# prepare labels for evaluating results\n",
    "if labeled:\n",
    "    y_cls = np.asarray(data['classifyLabels']).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and run classify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-build model\n",
    "\n",
    "inputs=Input(shape=(52, 52, 1))\n",
    "x = ConvOffset2D_train(1, name='conv_1_offset')(inputs)\n",
    "x = layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same', name='conv_1')(x)\n",
    "x = layers.BatchNormalization(axis=3, name='batch_normalization_1')(x)\n",
    "x = layers.Activation('relu', name='activation_1')(x)\n",
    "\n",
    "# Conv_2 layer\n",
    "x = ConvOffset2D_train(32, name='conv_2_offset')(x)\n",
    "x = layers.Conv2D(32*2, (3, 3), strides=(2, 2), padding='same', name='conv_2')(x)\n",
    "x = layers.BatchNormalization(axis=3, name='batch_normalization_2')(x)\n",
    "x = layers.Activation('relu', name='activation_2')(x)\n",
    "\n",
    "# Conv_3 layer\n",
    "x = ConvOffset2D_train(64, name='conv_3_offset')(x)\n",
    "x = layers.Conv2D(32*4, (3, 3), strides=(2, 2), padding='same', name='conv_3')(x)\n",
    "x = layers.BatchNormalization(axis=3, name='batch_normalization_3')(x)\n",
    "x = layers.Activation('relu', name='activation_3')(x)\n",
    "\n",
    "# Conv_4 layer\n",
    "x = ConvOffset2D_train(128, name='conv_4_offset')(x)\n",
    "x = layers.Conv2D(32*8, (3, 3), padding='same', name='conv_4')(x)\n",
    "x = layers.BatchNormalization(axis=3, name='batch_normalization_4')(x)\n",
    "x = layers.Activation('relu', name='activation_4')(x)\n",
    "\n",
    "# Conv_5 layer\n",
    "x = ConvOffset2D_train(256, name='conv_5_offset')(x)\n",
    "x = layers.Conv2D(32*4, (3, 3), strides=(2, 2), padding='same', name='conv_5')(x)\n",
    "x = layers.BatchNormalization(axis=3, name='batch_normalization_5')(x)\n",
    "x = layers.Activation('relu', name='activation_5')(x)\n",
    "\n",
    "# Pooling layer\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "\n",
    "# fc layer\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(9, activation='softmax', name='fc')(x)\n",
    "\n",
    "classify = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# load model weights\n",
    "classify.load_weights(classify_model)\n",
    "\n",
    "classify.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions\n",
    "start = time.time()\n",
    "\n",
    "classify_pred = classify.predict(x_cls)\n",
    "cls_labels = np.argmax(classify_pred, axis=1).astype(np.uint8)\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect tandem model results\n",
    "Saved predictions include 2 lists:\n",
    "- Output of classify model (softmax probabilities)\n",
    "- Labels predicted by classify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save predictions to local instance\n",
    "# predictions = [classify_pred, cls_labels]\n",
    "# with open(f'{result_path}/{result_filename}.pkl', \"wb\") as f:\n",
    "#     pickle.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF LABELED\n",
    "if labeled: \n",
    "    y_test = data['classifyLabels'].tolist()\n",
    "    \n",
    "    # manually compute overall accuracy\n",
    "    cls_cm = confusion_matrix(y_test, cls_labels)\n",
    "\n",
    "    cls_num = 0\n",
    "    for i in range(9):\n",
    "        cls_num += cls_cm[i][i]\n",
    "\n",
    "    overall_accuracy = cls_num / len(y_test) * 100\n",
    "    print(f'Overall Model Accuracy: {overall_accuracy:.2f}%') \n",
    "\n",
    "    # plot confusion matrix\n",
    "    helper.plot_confusion_matrix(y_test, cls_labels, mode='all', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF LABELED\n",
    "# plot confusion matrix counts\n",
    "if labeled:\n",
    "    helper.plot_confusion_matrix(y_test, cls_labels, mode='all', normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "colab_lenet_classify_all2.ipynb",
   "provenance": [
    {
     "file_id": "https://gist.github.com/mrgrhn/c6d2a157ebfc883e462f2d6e2ce2e3ce#file-lenet_tensorflow-ipynb",
     "timestamp": 1645028600895
    }
   ]
  },
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
