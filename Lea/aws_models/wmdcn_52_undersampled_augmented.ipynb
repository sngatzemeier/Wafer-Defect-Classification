{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deformable Convolution Network Model from Junliangwangdhu to classify all defects (including none)\n",
    "Implementation of the classification model from Junliangwangdhu [GitHub](https://github.com/Junliangwangdhu/WaferMap).\n",
    "\n",
    "Data:\n",
    "- Resized to 52x52 with no filter\n",
    "- Nones randomly undersampled to 30,000\n",
    "- Defects augmented with random flips and rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17003,
     "status": "ok",
     "timestamp": 1645130034371,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgzyTYobr3WBfcW_CbeVl1vthXNBKocBj7_WOpm=s64",
      "userId": "08012892845319420981"
     },
     "user_tz": 300
    },
    "id": "MMcWUHT5-eVD",
    "outputId": "00111a49-0a3c-4392-f905-805f1030b923"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras import datasets, layers, models, losses, optimizers, regularizers, callbacks, Input, Model, Sequential\n",
    "from keras import datasets, layers, models, losses, optimizers, regularizers, callbacks, Input, Model, Sequential\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from layers_train import ConvOffset2D_train\n",
    "\n",
    "import helpers as helper\n",
    "from keras_model_s3_wrapper import *\n",
    "\n",
    "import boto3\n",
    "import pickle5 as pickle\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'wafer-capstone'\n",
    "my_bucket = s3.Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify variables\n",
    "path = 'processed_data/dcn'\n",
    "filename = 'WM-clean-dcn52-undersampled-augmented'\n",
    "\n",
    "result_path = 'results_dcn'\n",
    "model_id = 'wmdcn'\n",
    "data_id = '52-undersampled-augmented'\n",
    "note = '' # -optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0.987 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ids', 'labels', 'dataset', 'wafermap']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "from io import BytesIO\n",
    "start = time.time()\n",
    "\n",
    "data_key = f'{path}/{filename}.npz'\n",
    "data_obj = my_bucket.Object(data_key).get()['Body'].read()\n",
    "data = np.load(BytesIO(data_obj), allow_pickle=True)\n",
    "\n",
    "print('Wall time: {:.3f} seconds'.format(time.time() - start))\n",
    "data.files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 842,
     "status": "ok",
     "timestamp": 1645130310224,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgzyTYobr3WBfcW_CbeVl1vthXNBKocBj7_WOpm=s64",
      "userId": "08012892845319420981"
     },
     "user_tz": 300
    },
    "id": "poThmiyAoHSi",
    "outputId": "b93c01b0-692b-4344-ce11-61da3613f166"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.924 seconds\n",
      "Train: (121065, 52, 52)\n",
      "Dev: (25942, 52, 52)\n",
      "Test: (25943, 52, 52)\n"
     ]
    }
   ],
   "source": [
    "# prepare inputs\n",
    "start = time.time()\n",
    "\n",
    "x_train = data['wafermap'][data['dataset']=='train']\n",
    "x_val = data['wafermap'][data['dataset']=='dev']\n",
    "x_test = data['wafermap'][data['dataset']=='test']\n",
    "\n",
    "print('Wall time: {:.3f} seconds'.format(time.time() - start))\n",
    "print(f'Train: {x_train.shape}')\n",
    "print(f'Dev: {x_val.shape}')\n",
    "print(f'Test: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0.00 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(121065, 52, 52, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expand tensor and create dummy dimension at axis 3\n",
    "# images in greyscale, so no channel dimension\n",
    "start = time.time()\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_val = np.expand_dims(x_val, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))\n",
    "# sanity check\n",
    "# expected: TensorShape([#rows, xdim, ydim, 1])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rIS8zJfn_aDO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0.04 seconds\n",
      "<class 'numpy.uint8'>\n",
      "0 0 0\n",
      "8 8 8\n"
     ]
    }
   ],
   "source": [
    "# prepare labels for supervised learning\n",
    "# note: make sure labels are integers if using sparse categorical cross entropy\n",
    "start = time.time()\n",
    "\n",
    "y_train = data['labels'][data['dataset']=='train']\n",
    "y_val = data['labels'][data['dataset']=='dev']\n",
    "y_test = data['labels'][data['dataset']=='test']\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))\n",
    "# sanity check\n",
    "# expected: type = int, min = 0, max = 8\n",
    "print(type(y_train[0]))\n",
    "print(min(y_train), min(y_val), min(y_test))\n",
    "print(max(y_train), max(y_val), max(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 562,
     "status": "ok",
     "timestamp": 1645130313542,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgzyTYobr3WBfcW_CbeVl1vthXNBKocBj7_WOpm=s64",
      "userId": "08012892845319420981"
     },
     "user_tz": 300
    },
    "id": "UeaLkOM-XRZA",
    "outputId": "31d7455b-87b6-42d6-e2ac-00c598e47744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/Wafer-Defect-Classification/Lea/aws_models/deform_conv.py:93: The name tf.ceil is deprecated. Please use tf.math.ceil instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 52, 52, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv_1_offset (ConvOffset2D_ (None, 52, 52, 1)         18        \n",
      "_________________________________________________________________\n",
      "conv_1 (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_2_offset (ConvOffset2D_ (None, 26, 26, 32)        18432     \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv2D)              (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 13, 13, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_3_offset (ConvOffset2D_ (None, 13, 13, 64)        73728     \n",
      "_________________________________________________________________\n",
      "conv_3 (Conv2D)              (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv_4_offset (ConvOffset2D_ (None, 7, 7, 128)         294912    \n",
      "_________________________________________________________________\n",
      "conv_4 (Conv2D)              (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv_5_offset (ConvOffset2D_ (None, 7, 7, 256)         1179648   \n",
      "_________________________________________________________________\n",
      "conv_5 (Conv2D)              (None, 4, 4, 128)         295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 2,253,211\n",
      "Trainable params: 2,251,995\n",
      "Non-trainable params: 1,216\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model architecture\n",
    "\n",
    "inputs=Input(shape=(52, 52, 1))\n",
    "x = ConvOffset2D_train(1, name='conv_1_offset')(inputs)\n",
    "x = layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same', name='conv_1')(x)\n",
    "x = layers.BatchNormalization(axis=3, name='batch_normalization_1')(x)\n",
    "x = layers.Activation('relu', name='activation_1')(x)\n",
    "\n",
    "# Conv_2 layer\n",
    "x = ConvOffset2D_train(32, name='conv_2_offset')(x)\n",
    "x = layers.Conv2D(32*2, (3, 3), strides=(2, 2), padding='same', name='conv_2')(x)\n",
    "x = layers.BatchNormalization(axis=3, name='batch_normalization_2')(x)\n",
    "x = layers.Activation('relu', name='activation_2')(x)\n",
    "\n",
    "# Conv_3 layer\n",
    "x = ConvOffset2D_train(64, name='conv_3_offset')(x)\n",
    "x = layers.Conv2D(32*4, (3, 3), strides=(2, 2), padding='same', name='conv_3')(x)\n",
    "x = layers.BatchNormalization(axis=3, name='batch_normalization_3')(x)\n",
    "x = layers.Activation('relu', name='activation_3')(x)\n",
    "\n",
    "# Conv_4 layer\n",
    "x = ConvOffset2D_train(128, name='conv_4_offset')(x)\n",
    "x = layers.Conv2D(32*8, (3, 3), padding='same', name='conv_4')(x)\n",
    "x = layers.BatchNormalization(axis=3, name='batch_normalization_4')(x)\n",
    "x = layers.Activation('relu', name='activation_4')(x)\n",
    "\n",
    "# Conv_5 layer\n",
    "x = ConvOffset2D_train(256, name='conv_5_offset')(x)\n",
    "x = layers.Conv2D(32*4, (3, 3), strides=(2, 2), padding='same', name='conv_5')(x)\n",
    "x = layers.BatchNormalization(axis=3, name='batch_normalization_5')(x)\n",
    "x = layers.Activation('relu', name='activation_5')(x)\n",
    "\n",
    "# Pooling layer\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "\n",
    "# fc layer\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(9, activation='softmax', name='fc')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pOAJW5ByhYCl"
   },
   "outputs": [],
   "source": [
    "# set model optimizer and metrics\n",
    "opt = optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=opt, loss=losses.sparse_categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 121065 samples, validate on 25942 samples\n",
      "Epoch 1/15\n",
      "121065/121065 [==============================] - 784s 6ms/step - loss: 0.2171 - accuracy: 0.9423 - val_loss: 0.1516 - val_accuracy: 0.9557\n",
      "Epoch 2/15\n",
      "121065/121065 [==============================] - 787s 7ms/step - loss: 0.1294 - accuracy: 0.9619 - val_loss: 0.1381 - val_accuracy: 0.9581\n",
      "Epoch 3/15\n",
      " 59904/121065 [=============>................] - ETA: 6:13 - loss: 0.1057 - accuracy: 0.9678"
     ]
    }
   ],
   "source": [
    "# run model\n",
    "start = time.time()\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=32, epochs=15)\n",
    "\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 896
    },
    "executionInfo": {
     "elapsed": 517,
     "status": "ok",
     "timestamp": 1645130877052,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgzyTYobr3WBfcW_CbeVl1vthXNBKocBj7_WOpm=s64",
      "userId": "08012892845319420981"
     },
     "user_tz": 300
    },
    "id": "AfX0d7zyocFw",
    "outputId": "6af8efe7-c57b-4b8e-d103-4334230e2663"
   },
   "outputs": [],
   "source": [
    "# visualize accuracy and loss history\n",
    "fig, axs = plt.subplots(2, 1, figsize=(15,15))\n",
    "\n",
    "axs[0].plot(history.history['loss'])\n",
    "axs[0].plot(history.history['val_loss'])\n",
    "axs[0].title.set_text('Training Loss vs Validation Loss')\n",
    "axs[0].legend(['Train', 'Val'])\n",
    "\n",
    "axs[1].plot(history.history['accuracy'])\n",
    "axs[1].plot(history.history['val_accuracy'])\n",
    "axs[1].title.set_text('Training Accuracy vs Validation Accuracy')\n",
    "axs[1].legend(['Train', 'Val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to S3\n",
    "s3_save_keras_model(model, f'{model_id}-{data_id}{note}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5266,
     "status": "ok",
     "timestamp": 1645130882314,
     "user": {
      "displayName": "Lea Cleary",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgzyTYobr3WBfcW_CbeVl1vthXNBKocBj7_WOpm=s64",
      "userId": "08012892845319420981"
     },
     "user_tz": 300
    },
    "id": "iLQqDljamZTV",
    "outputId": "93e90021-8340-4f3d-a9f8-ca7fd6fdb56d"
   },
   "outputs": [],
   "source": [
    "# compute model results on test set\n",
    "start = time.time()\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0_djoisO5ru"
   },
   "outputs": [],
   "source": [
    "# generate predictions for model analysis\n",
    "start = time.time()\n",
    "y_pred = model.predict(x_test)\n",
    "y_max = np.argmax(y_pred, axis=1).astype(np.uint8)\n",
    "predictions = [y_max, y_pred]\n",
    "print(\"Wall time: {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions to local instance\n",
    "with open(f'{result_path}/{model_id}-{data_id}{note}.pkl', \"wb\") as f:\n",
    "    pickle.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "helper.plot_confusion_matrix(y_test, y_max, mode='all', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix counts\n",
    "helper.plot_confusion_matrix(y_test, y_max, mode='all', normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "colab_lenet_classify_all2.ipynb",
   "provenance": [
    {
     "file_id": "https://gist.github.com/mrgrhn/c6d2a157ebfc883e462f2d6e2ce2e3ce#file-lenet_tensorflow-ipynb",
     "timestamp": 1645028600895
    }
   ]
  },
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
