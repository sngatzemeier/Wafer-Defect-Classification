{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpMyn5tnVkC6"
   },
   "source": [
    "#### Detect model from Yu, et al\n",
    "Implementation of the detection model from the Yu, et al [paper](https://drive.google.com/file/d/1nYl4w41CAcj8XwTEdVwcD5lVheUFIHVy/view?usp=sharing)\n",
    "\n",
    "Refinement on first run. Still tries to follow the paper as closely as possible, but some changes from the first run:\n",
    "- None type subsampled to keep only 30,000 random samples before resizing or filtering.\n",
    "- Data was first resized to 60x60 in preparation for using Tensorflow image functions.  Ultimately, the data was resized to 224x224, median filtered 7x7, and normalized using Tensorflow image functions.\n",
    "- Added last fully connected layer (FC3) that was accidentally omitted in the first run.\n",
    "- Saved both model and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pI8tDaXTVkC9"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMcWUHT5-eVD"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models, losses, optimizers\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle5 as pickle\n",
    "\n",
    "import helpers as helper\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from skimage.transform import resize as sk_resize\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZfNVVhPwVkC_"
   },
   "outputs": [],
   "source": [
    "# specify variables for model\n",
    "path = '../../data'\n",
    "filename = 'WM-clean2'\n",
    "option = '-detund' # -clsaug, -detund\n",
    "map_column = 'waferMap'\n",
    "label_column = 'detectLabels'\n",
    "filetype = 'pkl' # zip, pkl\n",
    "\n",
    "model_id = 'yudetect'\n",
    "result_path = '../../results'\n",
    "note = '-refined' # -optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qC9z2Ht7k0RL"
   },
   "outputs": [],
   "source": [
    "# load train, dev, and test sets\n",
    "start = time.time()\n",
    "\n",
    "if filetype == 'pkl':\n",
    "    # open pkl files\n",
    "    with open(f'{path}/{filename}-train{option}.pkl', \"rb\") as fh:\n",
    "        train = pickle.load(fh)\n",
    "    with open(f'{path}/{filename}-dev.pkl', \"rb\") as fh:\n",
    "        dev = pickle.load(fh)\n",
    "    with open(f'{path}/{filename}-test.pkl', \"rb\") as fh:\n",
    "        test = pickle.load(fh)\n",
    "\n",
    "elif filetype == 'zip':\n",
    "    train = helper.load(f'{path}/{filename}-train{option}.zip')\n",
    "    dev = helper.load(f'{path}/{filename}-dev.zip')\n",
    "    test = helper.load(f'{path}/{filename}-test.zip')\n",
    "\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "print()\n",
    "print(f\"Train: {len(train)}\")\n",
    "print(f\"Dev: {len(dev)}\")\n",
    "print(f\"Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhwQxQxmVkC_"
   },
   "source": [
    "#### Quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDQak4kDQzAW"
   },
   "outputs": [],
   "source": [
    "# baseline accuracy of test set\n",
    "nones = len(test[test.failureType == 'none'])\n",
    "total = len(test)\n",
    "print(f\"Baseline accuracy: {nones/total*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBKHNndkVkDA"
   },
   "outputs": [],
   "source": [
    "# train failure type distribution\n",
    "helper.defect_distribution(train, note='Train Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycx_tPxNVkDA"
   },
   "outputs": [],
   "source": [
    "# dev failure type distribution\n",
    "helper.defect_distribution(dev, note='Dev Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MjeGYFm0VkDB"
   },
   "outputs": [],
   "source": [
    "# test failure type distribution\n",
    "helper.defect_distribution(test, note='Test Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6gjrLT1VkDB"
   },
   "source": [
    "#### Data set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7apIV1oVkDB"
   },
   "outputs": [],
   "source": [
    "# resize images to 60x60 to use tensorflow utilities\n",
    "start = time.time()\n",
    "\n",
    "train['resized_map'] = train.waferMap.apply(lambda x: sk_resize(x, [60, 60], anti_aliasing=True))\n",
    "dev['resized_map'] = dev.waferMap.apply(lambda x: sk_resize(x, [60, 60], anti_aliasing=True))\n",
    "test['resized_map'] = test.waferMap.apply(lambda x: sk_resize(x, [60, 60], anti_aliasing=True))\n",
    "\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poThmiyAoHSi"
   },
   "outputs": [],
   "source": [
    "# prepare inputs\n",
    "start = time.time()\n",
    "\n",
    "x_train = np.stack(train['resized_map'])\n",
    "x_val = np.stack(dev['resized_map'])\n",
    "x_test = np.stack(test['resized_map'])\n",
    "\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "\n",
    "# sanity check\n",
    "# expected: (#rows, xdim, ydim)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hy_dVa44ZOLk"
   },
   "outputs": [],
   "source": [
    "# expand tensor and create dummy dimension at axis 3\n",
    "# images in greyscale, so no channel dimension\n",
    "start = time.time()\n",
    "\n",
    "x_train = tf.expand_dims(x_train, axis=3, name=None)\n",
    "x_val = tf.expand_dims(x_val, axis=3, name=None)\n",
    "x_test = tf.expand_dims(x_test, axis=3, name=None)\n",
    "\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "# sanity check\n",
    "# expected: TensorShape([#rows, xdim, ydim, 1])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpXtFXyJVkDC"
   },
   "outputs": [],
   "source": [
    "# resize image to 224x224\n",
    "start = time.time()\n",
    "\n",
    "x_train = tf.image.resize(x_train, [224,224], antialias=True)\n",
    "x_val = tf.image.resize(x_val, [224,224], antialias=True)\n",
    "x_test = tf.image.resize(x_test, [224,224], antialias=True)\n",
    "\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "# sanity check\n",
    "# expected: TensorShape([#rows, xdim, ydim, 1])\n",
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0e5qhI8kVkDC"
   },
   "outputs": [],
   "source": [
    "# apply 7x7 median filter\n",
    "start = time.time()\n",
    "\n",
    "x_train = tfa.image.median_filter2d(x_train, (7, 7))\n",
    "x_val = tfa.image.median_filter2d(x_val, (7, 7))\n",
    "x_test = tfa.image.median_filter2d(x_test, (7, 7))\n",
    "\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "# sanity check\n",
    "# expected: TensorShape([#rows, xdim, ydim, 1])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtD8pqj0VkDC"
   },
   "outputs": [],
   "source": [
    "# normalize images\n",
    "start = time.time()\n",
    "\n",
    "x_train = tf.image.per_image_standardization(x_train)\n",
    "x_val = tf.image.per_image_standardization(x_val)\n",
    "x_test = tf.image.per_image_standardization(x_test)\n",
    "\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "# sanity check\n",
    "# expected: TensorShape([#rows, xdim, ydim, 1])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIS8zJfn_aDO"
   },
   "outputs": [],
   "source": [
    "# prepare labels for supervised learning\n",
    "# note: make sure labels are integers if using sparse categorical cross entropy\n",
    "y_train = np.asarray(train[label_column]).astype('int64')\n",
    "y_val = np.asarray(dev[label_column]).astype('int64')\n",
    "y_test = np.asarray(test[label_column]).astype('int64')\n",
    "\n",
    "# sanity check\n",
    "# expected: type = int, min = 0, max = 1\n",
    "print(type(y_train[0]))\n",
    "print(min(y_train), min(y_val), min(y_test))\n",
    "print(max(y_train), max(y_val), max(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_Q3kbieVkDD"
   },
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UeaLkOM-XRZA"
   },
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=x_train.shape[1:]))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='sigmoid'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(2, activation='sigmoid'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOAJW5ByhYCl"
   },
   "outputs": [],
   "source": [
    "# set model optimizer and metrics\n",
    "opt = optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer='adam', loss=losses.sparse_categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xv-ml2R6hw_V"
   },
   "outputs": [],
   "source": [
    "# run model\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfX0d7zyocFw"
   },
   "outputs": [],
   "source": [
    "# visualize accuracy and loss history\n",
    "fig, axs = plt.subplots(2, 1, figsize=(15,15))\n",
    "\n",
    "axs[0].plot(history.history['loss'])\n",
    "axs[0].plot(history.history['val_loss'])\n",
    "axs[0].title.set_text('Training Loss vs Validation Loss')\n",
    "axs[0].legend(['Train', 'Val'])\n",
    "\n",
    "axs[1].plot(history.history['accuracy'])\n",
    "axs[1].plot(history.history['val_accuracy'])\n",
    "axs[1].title.set_text('Training Accuracy vs Validation Accuracy')\n",
    "axs[1].legend(['Train', 'Val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHwxW0_NVkDE"
   },
   "source": [
    "#### Model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLQqDljamZTV"
   },
   "outputs": [],
   "source": [
    "# compute model results on test set\n",
    "start = time.time()\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "print()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0_djoisO5ru"
   },
   "outputs": [],
   "source": [
    "# generate predictions for model analysis\n",
    "start = time.time()\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBt1AGCwVkDE"
   },
   "outputs": [],
   "source": [
    "# save model predictions\n",
    "with open(f'{result_path}/{model_id}-{filename}{option}{note}.pkl', \"wb\") as f:\n",
    "    pickle.dump(y_pred, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Cw-hqt1VkDF"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(f'{result_path}/{model_id}-{filename}{option}{note}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kqc9cWYBVkDF"
   },
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "helper.plot_confusion_matrix(y_test, y_pred, mode='detect', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19YwIGHnVkDF"
   },
   "outputs": [],
   "source": [
    "# plot confusion matrix counts\n",
    "helper.plot_confusion_matrix(y_test, y_pred, mode='detect', normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFcGl3HVVkDF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "yu_detect_refined.ipynb",
   "provenance": [
    {
     "file_id": "https://gist.github.com/mrgrhn/c6d2a157ebfc883e462f2d6e2ce2e3ce#file-lenet_tensorflow-ipynb",
     "timestamp": 1645028600895
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
