{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STzyyTCHVWsP"
   },
   "source": [
    "#### Classify model from Yu, et al\n",
    "Implementation of the classification model from the Yu, et al [paper](https://drive.google.com/file/d/1nYl4w41CAcj8XwTEdVwcD5lVheUFIHVy/view?usp=sharing)\n",
    "\n",
    "Refinement on first run. Data was resized to 224x224 and median filtered 7x7. Classes were augmented to max class count using random flips and rotations.\n",
    "\n",
    "Still tries to follow the paper as closely as possible, but some changes from the first run:\n",
    "- Added last fully connected layer (FC3) that was accidentally omitted in the first run.\n",
    "- Added second dropout layer that was accidentally omitted in first run (dropout applied to FC1 and FC2 instead of just FC2).\n",
    "- Adjusted L2 regularization to 0.001 instead of 0.000001 for both FC1 and FC2 (overfitting observed in first run).\n",
    "- Saved both model and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMcWUHT5-eVD"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models, losses, optimizers, regularizers\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle5 as pickle\n",
    "\n",
    "import helpers as helper\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgCMfZV3VWsW"
   },
   "outputs": [],
   "source": [
    "# specify variables for model\n",
    "path = '../../../data/resized224'\n",
    "filename = 'WM-clean-id224filter7'\n",
    "option = '-clsaug' # -clsaug, -detund\n",
    "map_column = 'filterMap7'\n",
    "label_column = 'classifyLabels'\n",
    "filetype = 'zip' # zip, pkl\n",
    "\n",
    "model_id = 'yuclassify'\n",
    "result_path = '../../results'\n",
    "note = '-refined2' # -optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qC9z2Ht7k0RL"
   },
   "outputs": [],
   "source": [
    "# load train, dev, and test sets\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "if filetype == 'pkl':\n",
    "    # open pkl files\n",
    "    with open(f'{path}/{filename}-train{option}.pkl', \"rb\") as fh:\n",
    "        train = pickle.load(fh)\n",
    "    with open(f'{path}/{filename}-dev.pkl', \"rb\") as fh:\n",
    "        dev = pickle.load(fh)\n",
    "    with open(f'{path}/{filename}-test.pkl', \"rb\") as fh:\n",
    "        test = pickle.load(fh)\n",
    "\n",
    "elif filetype == 'zip':\n",
    "    train = helper.load(f'{path}/{filename}-train{option}.zip')\n",
    "    dev = helper.load(f'{path}/{filename}-dev.zip')\n",
    "    test = helper.load(f'{path}/{filename}-test.zip')\n",
    "\n",
    "# remove none type from dev and test sets\n",
    "dev = dev[dev.failureType != 'none']\n",
    "test = test[test.failureType != 'none']\n",
    "\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "print(f\"Train: {len(train)}\")\n",
    "print(f\"Dev: {len(dev)}\")\n",
    "print(f\"Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3nog5S5VWsX"
   },
   "source": [
    "#### Quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDQak4kDQzAW"
   },
   "outputs": [],
   "source": [
    "# baseline accuracy of test set\n",
    "nones = len(test[test.failureType == 'Edge-Ring'])\n",
    "total = len(test)\n",
    "print(f\"Baseline accuracy: {nones/total*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PG2G7m_RVWsY"
   },
   "outputs": [],
   "source": [
    "# train failure type distribution\n",
    "helper.defect_distribution(train, note='Train Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3NvpxC3VWsZ"
   },
   "outputs": [],
   "source": [
    "# dev failure type distribution\n",
    "helper.defect_distribution(dev, note='Dev Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVfsOyGXVWsZ"
   },
   "outputs": [],
   "source": [
    "# test failure type distribution\n",
    "helper.defect_distribution(test, note='Test Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Z5O_8IiVWsa"
   },
   "source": [
    "#### Data set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poThmiyAoHSi"
   },
   "outputs": [],
   "source": [
    "# prepare inputs\n",
    "start = time.time()\n",
    "\n",
    "x_train = np.stack(train[map_column])\n",
    "x_val = np.stack(dev[map_column])\n",
    "x_test = np.stack(test[map_column])\n",
    "\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "\n",
    "# sanity check\n",
    "# expected: (#rows, xdim, ydim)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hy_dVa44ZOLk"
   },
   "outputs": [],
   "source": [
    "# expand tensor and create dummy dimension at axis 3\n",
    "# images in greyscale, so no channel dimension\n",
    "start = time.time()\n",
    "\n",
    "x_train = tf.expand_dims(x_train, axis=3, name=None)\n",
    "x_val = tf.expand_dims(x_val, axis=3, name=None)\n",
    "x_test = tf.expand_dims(x_test, axis=3, name=None)\n",
    "\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "\n",
    "# sanity check\n",
    "# expected: TensorShape([#rows, xdim, ydim, 1])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIS8zJfn_aDO"
   },
   "outputs": [],
   "source": [
    "# prepare labels for supervised learning\n",
    "# note: make sure labels are integers if using sparse categorical cross entropy\n",
    "y_train = np.asarray(train[label_column]).astype('int64')\n",
    "y_val = np.asarray(dev[label_column]).astype('int64')\n",
    "y_test = np.asarray(test[label_column]).astype('int64')\n",
    "\n",
    "# subtract 1 from labels to agree with model\n",
    "y_train = y_train - 1\n",
    "y_val = y_val - 1\n",
    "y_test = y_test - 1\n",
    "\n",
    "# sanity check\n",
    "print(type(y_train[0]))\n",
    "print(min(y_train), min(y_val), min(y_test))\n",
    "print(max(y_train), max(y_val), max(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaQTa7frVWsb"
   },
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UeaLkOM-XRZA"
   },
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=x_train.shape[1:]))\n",
    "model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "model.add(layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "model.add(layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "model.add(layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(4096, activation='sigmoid', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1024, activation='sigmoid', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(8, activation='sigmoid', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(layers.Dense(8, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOAJW5ByhYCl"
   },
   "outputs": [],
   "source": [
    "# set model optimizer and metrics\n",
    "opt = optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer='adam', loss=losses.sparse_categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xv-ml2R6hw_V"
   },
   "outputs": [],
   "source": [
    "# run model\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfX0d7zyocFw"
   },
   "outputs": [],
   "source": [
    "# visualize accuracy and loss history\n",
    "fig, axs = plt.subplots(2, 1, figsize=(15,15))\n",
    "\n",
    "axs[0].plot(history.history['loss'])\n",
    "axs[0].plot(history.history['val_loss'])\n",
    "axs[0].title.set_text('Training Loss vs Validation Loss')\n",
    "axs[0].legend(['Train', 'Val'])\n",
    "\n",
    "axs[1].plot(history.history['accuracy'])\n",
    "axs[1].plot(history.history['val_accuracy'])\n",
    "axs[1].title.set_text('Training Accuracy vs Validation Accuracy')\n",
    "axs[1].legend(['Train', 'Val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kp7SVF-zVWsd"
   },
   "source": [
    "#### Model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLQqDljamZTV"
   },
   "outputs": [],
   "source": [
    "# compute model results on test set\n",
    "start = time.time()\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))\n",
    "print()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0_djoisO5ru"
   },
   "outputs": [],
   "source": [
    "# generate predictions for model analysis\n",
    "start = time.time()\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6i3jEevVWse"
   },
   "outputs": [],
   "source": [
    "# save results, history, and predictions\n",
    "with open(f'{result_path}/{model_id}-{filename}{option}{note}.pkl', \"wb\") as f:\n",
    "    pickle.dump(y_pred, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l284RltNVWse"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(f'{result_path}/{model_id}-{filename}{option}{note}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9hY8w4iVWse"
   },
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "helper.plot_confusion_matrix(y_test, y_pred, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t3DeyT82VWse"
   },
   "outputs": [],
   "source": [
    "# plot confusion matrix counts\n",
    "helper.plot_confusion_matrix(y_test, y_pred, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1Vj3vJpVWsf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "yu_classify_refined2.ipynb",
   "provenance": [
    {
     "file_id": "https://gist.github.com/mrgrhn/c6d2a157ebfc883e462f2d6e2ce2e3ce#file-lenet_tensorflow-ipynb",
     "timestamp": 1645028600895
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
